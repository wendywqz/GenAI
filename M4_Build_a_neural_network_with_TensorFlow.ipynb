{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ+eTnxNX+rm8n49qt4FJS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wendywqz/GenAI/blob/main/M4_Build_a_neural_network_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 Load the dataset"
      ],
      "metadata": {
        "id": "wFA23J2jy1Yf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TmJzfmtpyHUg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values between 0 and 1\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizing** pixel values to a range of 0 to 1 ensures efficient model training and helps avoid issues with large input values that can negatively impact the optimization process."
      ],
      "metadata": {
        "id": "a7hDmxG8y6dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 Define the neural network model"
      ],
      "metadata": {
        "id": "5slZqq7LzaJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the model\n",
        "model = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)), # Input layer that flattens the image\n",
        "    layers.Dense(128, activation='relu'), # Hidden layer with 128 neurons\n",
        "    layers.Dense(10, activation='softmax') # Output layer with 10 classes\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bof8p-hzeVr",
        "outputId": "ae3cc1b8-92d8-4329-9793-04f325672ba1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flatten layer:** This transforms each 28 × 28 image into a 1D array of 784 values, preparing it for the fully connected layers.\n",
        "\n",
        "**Dense layers:** The hidden layer with 128 neurons and ReLU activation adds nonlinearity, allowing the model to capture complex patterns. The output layer uses softmax activation to produce probabilities for each of the 10 classes"
      ],
      "metadata": {
        "id": "cvDF6j9Uz8fN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 Compile the model"
      ],
      "metadata": {
        "id": "itJwkeSt0AZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "LSrmpieQ0FwY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adam optimizer:** Adam is a popular optimizer for deep learning models due to its adaptive learning rate, which often leads to faster convergence.\n",
        "\n",
        "**Sparse categorical cross-entropy: **This is a suitable loss function when dealing with integer labels (i.e., when each label is a class index).\n",
        "\n",
        "**Accuracy metric:** The accuracy metric helps you monitor how well the model is predicting the correct classes during training and evaluation."
      ],
      "metadata": {
        "id": "CzNIaDW40wz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 Train the model"
      ],
      "metadata": {
        "id": "QFIDHux305xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb_PVNiE04oT",
        "outputId": "4016f310-93b8-4b27-d853-b8b0c1b3286b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7769 - loss: 0.6369\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8600 - loss: 0.3897\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8795 - loss: 0.3336\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8842 - loss: 0.3179\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8934 - loss: 0.2897\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.8961 - loss: 0.2822\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9035 - loss: 0.2630\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9043 - loss: 0.2542\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9080 - loss: 0.2445\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9114 - loss: 0.2375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78f19398f3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epochs: This refers to the number of times the model sees the entire training dataset. Training for 10 epochs is generally a good start for this problem.\n",
        "\n",
        "Batch size: A batch size of 32 means the model updates its weights after processing 32 samples, balancing computational efficiency and model performance."
      ],
      "metadata": {
        "id": "ksJ9s8_T1MTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 Evaluate the model"
      ],
      "metadata": {
        "id": "rvf9AQxY1Oub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the mdoel on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "# expectation: 85% ~ 90%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kncIaIIM1SaT",
        "outputId": "268f24d8-2d81-42a0-dc97-6f50e7c95e12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8835 - loss: 0.3510\n",
            "Test accuracy: 0.8835999965667725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test loss and accuracy: The test loss indicates how well the model's predictions match the true labels on the test data, while the test accuracy shows the percentage of correct predictions."
      ],
      "metadata": {
        "id": "F1cPQadV10hY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimentation\n",
        "\n",
        "- Adding more hidden layers to make the model deeper and potentially more accurate.\n",
        "- Increasing or decreasing the number of neurons in the hidden layer to adjust the model's capacity.\n",
        "- Changing the activation function to something other than ReLU, such as sigmoid or tanh, and observing the impact.\n",
        "- Using a different optimizer, such as stochastic gradient descent (SGD), and comparing its performance to Adam."
      ],
      "metadata": {
        "id": "K7w6ejzR13R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='sigmoid'),\n",
        "    layers.Dense(64, activation='sigmoid'), # New hidden layer with 64 neurons\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "WzjKGP3Y2HNf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "DOffvll55WgK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9PSNjkF5XRG",
        "outputId": "073f8b38-89fc-474d-d195-5f882a6bbfb8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7172 - loss: 0.9049\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8545 - loss: 0.4045\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8707 - loss: 0.3503\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8834 - loss: 0.3254\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8885 - loss: 0.3071\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8913 - loss: 0.2948\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8981 - loss: 0.2796\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8992 - loss: 0.2731\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9050 - loss: 0.2570\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9083 - loss: 0.2493\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78f160f288d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the mdoel on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "# expectation: 85% ~ 90%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsRb2Gyb5dkL",
        "outputId": "1b77fff8-43f5-45b7-9e1a-434ad886ab9a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8832 - loss: 0.3263\n",
            "Test accuracy: 0.8823999762535095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码使用 Keras 构建了一个名为 model 的神经网络模型。让我们逐行分析：\n",
        "\n",
        "model = models.Sequential(...): 这行代码创建了一个 Sequential 模型，它表示网络层按顺序排列。\n",
        "layers.Flatten(input_shape=(28, 28)): 这是一个扁平化层，它将输入的 28x28 的图像转换为一个 784 维的一维向量。这是为了让数据能够被后续的全连接层处理。\n",
        "layers.Dense(128, activation='sigmoid'): 这是一个全连接层，它包含 128 个神经元，并使用 Sigmoid 激活函数。它将前一层（扁平化层）的输出作为输入，并进行线性变换，然后应用 Sigmoid 函数来引入非线性。\n",
        "layers.Dense(64, activation='sigmoid'): 这是另一个全连接层，包含 64 个神经元，也使用 Sigmoid 激活函数。它接收前一层（128 个神经元的全连接层）的输出作为输入。\n",
        "layers.Dense(10, activation='softmax'): 这是输出层，包含 10 个神经元，并使用 Softmax 激活函数。它用于将模型的输出转换为对应于 10 个类别（例如，0-9 的数字）的概率分布。\n",
        "将 ReLU 替换为 Sigmoid 的影响\n",
        "将激活函数从 ReLU 替换为 Sigmoid 会对模型的训练和性能产生以下影响：\n",
        "\n",
        "1. 梯度消失问题: Sigmoid 函数在输入值较大或较小时，其导数接近于 0。这会导致在反向传播过程中梯度消失，从而减缓模型的训练速度，甚至导致模型无法收敛到最优解。ReLU 函数在正值区域的导数为 1，可以有效避免梯度消失问题。\n",
        "2. 输出范围: Sigmoid 函数的输出范围在 0 到 1 之间，而 ReLU 函数的输出范围在 0 到正无穷之间。这可能会影响模型的表达能力和学习到的特征的范围。\n",
        "3. 模型性能: 在大多数情况下，ReLU 函数在深度学习模型中表现更好，因为它可以避免梯度消失问题，并允许模型学习更复杂的特征。然而，对于某些特定任务，Sigmoid 函数可能更合适。\n",
        "\n",
        "总结: 将激活函数从 ReLU 替换为 Sigmoid 通常会导致模型训练速度变慢，并且可能降低模型的性能，尤其是对于较深的网络。但是，对于某些特定任务，Sigmoid 函数可能仍然是一个可行的选择。建议您根据实际情况进行实验，并比较不同激活函数的性能。"
      ],
      "metadata": {
        "id": "My4-lvU78U1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**把relu换成sigmoid** Testing accuracy反而高， 这是一个很好的问题！理论上来说，ReLU 在深度学习中通常比 Sigmoid 表现更好，尤其是在避免梯度消失和允许模型学习更复杂特征方面。然而，在你的实验中，使用 Sigmoid 激活函数反而得到了更高的测试准确率，这可能由以下几个因素导致：\n",
        "\n",
        "- 数据集特性: Fashion MNIST 数据集相对简单，可能不需要太复杂的特征表示。在这种情况下，Sigmoid 的非线性能力足以完成任务，而 ReLU 的额外复杂性可能反而导致过拟合，降低泛化能力。\n",
        "- 网络结构: 你在实验中使用了两个隐藏层，并且第二个隐藏层的维度较小 (64)。这种网络结构可能更适合 Sigmoid 激活函数，因为它可以防止梯度消失问题在浅层网络中变得过于严重。\n",
        "- 随机性: 神经网络训练过程中存在一定的随机性，包括权重初始化、数据 shuffle 等。这可能导致即使使用相同的模型结构和超参数，每次训练的结果也会略有不同。因此，一次实验结果的差异可能只是随机性造成的，并不能完全代表两种激活函数的真实性能差异。\n",
        "- 其他因素: 除了激活函数外，还有许多其他因素会影响模型的性能，例如学习率、优化器、正则化方法等。这些因素的差异也可能导致最终结果的不同。\n",
        "\n",
        "为了更全面地比较 ReLU 和 Sigmoid 的性能，建议你进行以下实验：\n",
        "\n",
        "1. 多次实验: 多次运行实验，并记录每次的测试准确率，然后比较两种激活函数的平均性能和方差。\n",
        "2. 调整超参数: 尝试调整学习率、优化器、正则化方法等超参数，看看它们对两种激活函数的影响。\n",
        "更复杂的网络结构: 尝试使用更深的网络结构或更大的隐藏层维度，看看 ReLU 是否在更复杂的模型中表现更好。\n",
        "- 通过这些实验，你可以更深入地了解 ReLU 和 Sigmoid 在 Fashion MNIST 数据集上的性能差异，并为你的模型选择最合适的激活函数。\n",
        "\n",
        "总而言之，虽然 ReLU 通常被认为是深度学习中更好的激活函数，但在某些特定情况下，Sigmoid 也可能表现更好。在实践中，建议你根据具体任务和数据集特性进行实验，并选择最合适的激活函数。"
      ],
      "metadata": {
        "id": "tms8Aorx8X1N"
      }
    }
  ]
}